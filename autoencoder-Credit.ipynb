{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":false},"source":["# Autoencoder Architecture\n","\n","## Introduction\n","Autoencoder is a kind of Deep Learning architectures. Autoencoder architecture encompasses two sub-systems as encoder and decoder. Both these sub-systems are made up of independent Neural Network with a defined set of layers and activation functions. The fundamental characteristic feature of Autoencoder architecture is extracting the latent(hidden) data points from the given dataset. \n","\n","This notebook is created out of inspiration from the post on [Geekforgeek](https://www.geeksforgeeks.org/ml-classifying-data-using-an-auto-encoder/)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset-Credit card transactions\n","\n","The dataset we're going to use in this kernel is `creditcard.csv` which basically a credit card transactions in the past. Using an encoder-decorder system we will find the hidden data points and apply a linear classifier to detect the Fraud(1) or Genuine/not-fraud (0) credit card transactions. \n","\n","## Import dependent libraries"]},{"cell_type":"code","execution_count":49,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","\n","# Plotting libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# SKLearn related libraries\n","from sklearn.manifold import TSNE\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import RobustScaler\n","\n","# Classifiers\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","\n","# Metrics\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","\n","# Keras NN related libraries\n","from keras import layers\n","from keras.layers import Input, Dense\n","from keras.models import Model, Sequential \n","from keras import regularizers"]},{"cell_type":"markdown","metadata":{},"source":["## Loading Dataset"]},{"cell_type":"code","execution_count":50,"metadata":{"trusted":true},"outputs":[],"source":["data_path = r\"C:\\Users\\bodar\\Documents\\creditcard.csv\"\n","\n","# print(os.path.exists(data_path))\n","\n","# Load the data\n","card_df = pd.read_csv(data_path, header=0)"]},{"cell_type":"code","execution_count":51,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 284807 entries, 0 to 284806\n","Data columns (total 31 columns):\n"," #   Column  Non-Null Count   Dtype  \n","---  ------  --------------   -----  \n"," 0   Time    284807 non-null  float64\n"," 1   V1      284807 non-null  float64\n"," 2   V2      284807 non-null  float64\n"," 3   V3      284807 non-null  float64\n"," 4   V4      284807 non-null  float64\n"," 5   V5      284807 non-null  float64\n"," 6   V6      284807 non-null  float64\n"," 7   V7      284807 non-null  float64\n"," 8   V8      284807 non-null  float64\n"," 9   V9      284807 non-null  float64\n"," 10  V10     284807 non-null  float64\n"," 11  V11     284807 non-null  float64\n"," 12  V12     284807 non-null  float64\n"," 13  V13     284807 non-null  float64\n"," 14  V14     284807 non-null  float64\n"," 15  V15     284807 non-null  float64\n"," 16  V16     284807 non-null  float64\n"," 17  V17     284807 non-null  float64\n"," 18  V18     284807 non-null  float64\n"," 19  V19     284807 non-null  float64\n"," 20  V20     284807 non-null  float64\n"," 21  V21     284807 non-null  float64\n"," 22  V22     284807 non-null  float64\n"," 23  V23     284807 non-null  float64\n"," 24  V24     284807 non-null  float64\n"," 25  V25     284807 non-null  float64\n"," 26  V26     284807 non-null  float64\n"," 27  V27     284807 non-null  float64\n"," 28  V28     284807 non-null  float64\n"," 29  Amount  284807 non-null  float64\n"," 30  Class   284807 non-null  int64  \n","dtypes: float64(30), int64(1)\n","memory usage: 67.4 MB\n","========================================================================================================================\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>Amount</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>-1.359807</td>\n","      <td>-0.072781</td>\n","      <td>2.536347</td>\n","      <td>1.378155</td>\n","      <td>-0.338321</td>\n","      <td>0.462388</td>\n","      <td>0.239599</td>\n","      <td>0.098698</td>\n","      <td>0.363787</td>\n","      <td>...</td>\n","      <td>-0.018307</td>\n","      <td>0.277838</td>\n","      <td>-0.110474</td>\n","      <td>0.066928</td>\n","      <td>0.128539</td>\n","      <td>-0.189115</td>\n","      <td>0.133558</td>\n","      <td>-0.021053</td>\n","      <td>149.62</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.191857</td>\n","      <td>0.266151</td>\n","      <td>0.166480</td>\n","      <td>0.448154</td>\n","      <td>0.060018</td>\n","      <td>-0.082361</td>\n","      <td>-0.078803</td>\n","      <td>0.085102</td>\n","      <td>-0.255425</td>\n","      <td>...</td>\n","      <td>-0.225775</td>\n","      <td>-0.638672</td>\n","      <td>0.101288</td>\n","      <td>-0.339846</td>\n","      <td>0.167170</td>\n","      <td>0.125895</td>\n","      <td>-0.008983</td>\n","      <td>0.014724</td>\n","      <td>2.69</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>-1.358354</td>\n","      <td>-1.340163</td>\n","      <td>1.773209</td>\n","      <td>0.379780</td>\n","      <td>-0.503198</td>\n","      <td>1.800499</td>\n","      <td>0.791461</td>\n","      <td>0.247676</td>\n","      <td>-1.514654</td>\n","      <td>...</td>\n","      <td>0.247998</td>\n","      <td>0.771679</td>\n","      <td>0.909412</td>\n","      <td>-0.689281</td>\n","      <td>-0.327642</td>\n","      <td>-0.139097</td>\n","      <td>-0.055353</td>\n","      <td>-0.059752</td>\n","      <td>378.66</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>-0.966272</td>\n","      <td>-0.185226</td>\n","      <td>1.792993</td>\n","      <td>-0.863291</td>\n","      <td>-0.010309</td>\n","      <td>1.247203</td>\n","      <td>0.237609</td>\n","      <td>0.377436</td>\n","      <td>-1.387024</td>\n","      <td>...</td>\n","      <td>-0.108300</td>\n","      <td>0.005274</td>\n","      <td>-0.190321</td>\n","      <td>-1.175575</td>\n","      <td>0.647376</td>\n","      <td>-0.221929</td>\n","      <td>0.062723</td>\n","      <td>0.061458</td>\n","      <td>123.50</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.0</td>\n","      <td>-1.158233</td>\n","      <td>0.877737</td>\n","      <td>1.548718</td>\n","      <td>0.403034</td>\n","      <td>-0.407193</td>\n","      <td>0.095921</td>\n","      <td>0.592941</td>\n","      <td>-0.270533</td>\n","      <td>0.817739</td>\n","      <td>...</td>\n","      <td>-0.009431</td>\n","      <td>0.798278</td>\n","      <td>-0.137458</td>\n","      <td>0.141267</td>\n","      <td>-0.206010</td>\n","      <td>0.502292</td>\n","      <td>0.219422</td>\n","      <td>0.215153</td>\n","      <td>69.99</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 31 columns</p>\n","</div>"],"text/plain":["   Time        V1        V2        V3        V4        V5        V6        V7  \\\n","0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n","1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n","2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n","3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n","4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n","\n","         V8        V9  ...       V21       V22       V23       V24       V25  \\\n","0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n","1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n","2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n","3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n","4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n","\n","        V26       V27       V28  Amount  Class  \n","0 -0.189115  0.133558 -0.021053  149.62      0  \n","1  0.125895 -0.008983  0.014724    2.69      0  \n","2 -0.139097 -0.055353 -0.059752  378.66      0  \n","3 -0.221929  0.062723  0.061458  123.50      0  \n","4  0.502292  0.219422  0.215153   69.99      0  \n","\n","[5 rows x 31 columns]"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["card_df.info()\n","print(\"====\"*30)\n","card_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Exploratory Data Analysis\n","\n","1. Exploring on statistics information about the data"]},{"cell_type":"code","execution_count":52,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>min</th>\n","      <th>25%</th>\n","      <th>50%</th>\n","      <th>75%</th>\n","      <th>max</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Time</th>\n","      <td>284807.0</td>\n","      <td>9.481386e+04</td>\n","      <td>47488.145955</td>\n","      <td>0.000000</td>\n","      <td>54201.500000</td>\n","      <td>84692.000000</td>\n","      <td>139320.500000</td>\n","      <td>172792.000000</td>\n","    </tr>\n","    <tr>\n","      <th>V1</th>\n","      <td>284807.0</td>\n","      <td>3.918649e-15</td>\n","      <td>1.958696</td>\n","      <td>-56.407510</td>\n","      <td>-0.920373</td>\n","      <td>0.018109</td>\n","      <td>1.315642</td>\n","      <td>2.454930</td>\n","    </tr>\n","    <tr>\n","      <th>V2</th>\n","      <td>284807.0</td>\n","      <td>5.682686e-16</td>\n","      <td>1.651309</td>\n","      <td>-72.715728</td>\n","      <td>-0.598550</td>\n","      <td>0.065486</td>\n","      <td>0.803724</td>\n","      <td>22.057729</td>\n","    </tr>\n","    <tr>\n","      <th>V3</th>\n","      <td>284807.0</td>\n","      <td>-8.761736e-15</td>\n","      <td>1.516255</td>\n","      <td>-48.325589</td>\n","      <td>-0.890365</td>\n","      <td>0.179846</td>\n","      <td>1.027196</td>\n","      <td>9.382558</td>\n","    </tr>\n","    <tr>\n","      <th>V4</th>\n","      <td>284807.0</td>\n","      <td>2.811118e-15</td>\n","      <td>1.415869</td>\n","      <td>-5.683171</td>\n","      <td>-0.848640</td>\n","      <td>-0.019847</td>\n","      <td>0.743341</td>\n","      <td>16.875344</td>\n","    </tr>\n","    <tr>\n","      <th>V5</th>\n","      <td>284807.0</td>\n","      <td>-1.552103e-15</td>\n","      <td>1.380247</td>\n","      <td>-113.743307</td>\n","      <td>-0.691597</td>\n","      <td>-0.054336</td>\n","      <td>0.611926</td>\n","      <td>34.801666</td>\n","    </tr>\n","    <tr>\n","      <th>V6</th>\n","      <td>284807.0</td>\n","      <td>2.040130e-15</td>\n","      <td>1.332271</td>\n","      <td>-26.160506</td>\n","      <td>-0.768296</td>\n","      <td>-0.274187</td>\n","      <td>0.398565</td>\n","      <td>73.301626</td>\n","    </tr>\n","    <tr>\n","      <th>V7</th>\n","      <td>284807.0</td>\n","      <td>-1.698953e-15</td>\n","      <td>1.237094</td>\n","      <td>-43.557242</td>\n","      <td>-0.554076</td>\n","      <td>0.040103</td>\n","      <td>0.570436</td>\n","      <td>120.589494</td>\n","    </tr>\n","    <tr>\n","      <th>V8</th>\n","      <td>284807.0</td>\n","      <td>-1.893285e-16</td>\n","      <td>1.194353</td>\n","      <td>-73.216718</td>\n","      <td>-0.208630</td>\n","      <td>0.022358</td>\n","      <td>0.327346</td>\n","      <td>20.007208</td>\n","    </tr>\n","    <tr>\n","      <th>V9</th>\n","      <td>284807.0</td>\n","      <td>-3.147640e-15</td>\n","      <td>1.098632</td>\n","      <td>-13.434066</td>\n","      <td>-0.643098</td>\n","      <td>-0.051429</td>\n","      <td>0.597139</td>\n","      <td>15.594995</td>\n","    </tr>\n","    <tr>\n","      <th>V10</th>\n","      <td>284807.0</td>\n","      <td>1.772925e-15</td>\n","      <td>1.088850</td>\n","      <td>-24.588262</td>\n","      <td>-0.535426</td>\n","      <td>-0.092917</td>\n","      <td>0.453923</td>\n","      <td>23.745136</td>\n","    </tr>\n","    <tr>\n","      <th>V11</th>\n","      <td>284807.0</td>\n","      <td>9.289524e-16</td>\n","      <td>1.020713</td>\n","      <td>-4.797473</td>\n","      <td>-0.762494</td>\n","      <td>-0.032757</td>\n","      <td>0.739593</td>\n","      <td>12.018913</td>\n","    </tr>\n","    <tr>\n","      <th>V12</th>\n","      <td>284807.0</td>\n","      <td>-1.803266e-15</td>\n","      <td>0.999201</td>\n","      <td>-18.683715</td>\n","      <td>-0.405571</td>\n","      <td>0.140033</td>\n","      <td>0.618238</td>\n","      <td>7.848392</td>\n","    </tr>\n","    <tr>\n","      <th>V13</th>\n","      <td>284807.0</td>\n","      <td>1.674888e-15</td>\n","      <td>0.995274</td>\n","      <td>-5.791881</td>\n","      <td>-0.648539</td>\n","      <td>-0.013568</td>\n","      <td>0.662505</td>\n","      <td>7.126883</td>\n","    </tr>\n","    <tr>\n","      <th>V14</th>\n","      <td>284807.0</td>\n","      <td>1.475621e-15</td>\n","      <td>0.958596</td>\n","      <td>-19.214325</td>\n","      <td>-0.425574</td>\n","      <td>0.050601</td>\n","      <td>0.493150</td>\n","      <td>10.526766</td>\n","    </tr>\n","    <tr>\n","      <th>V15</th>\n","      <td>284807.0</td>\n","      <td>3.501098e-15</td>\n","      <td>0.915316</td>\n","      <td>-4.498945</td>\n","      <td>-0.582884</td>\n","      <td>0.048072</td>\n","      <td>0.648821</td>\n","      <td>8.877742</td>\n","    </tr>\n","    <tr>\n","      <th>V16</th>\n","      <td>284807.0</td>\n","      <td>1.392460e-15</td>\n","      <td>0.876253</td>\n","      <td>-14.129855</td>\n","      <td>-0.468037</td>\n","      <td>0.066413</td>\n","      <td>0.523296</td>\n","      <td>17.315112</td>\n","    </tr>\n","    <tr>\n","      <th>V17</th>\n","      <td>284807.0</td>\n","      <td>-7.466538e-16</td>\n","      <td>0.849337</td>\n","      <td>-25.162799</td>\n","      <td>-0.483748</td>\n","      <td>-0.065676</td>\n","      <td>0.399675</td>\n","      <td>9.253526</td>\n","    </tr>\n","    <tr>\n","      <th>V18</th>\n","      <td>284807.0</td>\n","      <td>4.258754e-16</td>\n","      <td>0.838176</td>\n","      <td>-9.498746</td>\n","      <td>-0.498850</td>\n","      <td>-0.003636</td>\n","      <td>0.500807</td>\n","      <td>5.041069</td>\n","    </tr>\n","    <tr>\n","      <th>V19</th>\n","      <td>284807.0</td>\n","      <td>9.019919e-16</td>\n","      <td>0.814041</td>\n","      <td>-7.213527</td>\n","      <td>-0.456299</td>\n","      <td>0.003735</td>\n","      <td>0.458949</td>\n","      <td>5.591971</td>\n","    </tr>\n","    <tr>\n","      <th>V20</th>\n","      <td>284807.0</td>\n","      <td>5.126845e-16</td>\n","      <td>0.770925</td>\n","      <td>-54.497720</td>\n","      <td>-0.211721</td>\n","      <td>-0.062481</td>\n","      <td>0.133041</td>\n","      <td>39.420904</td>\n","    </tr>\n","    <tr>\n","      <th>V21</th>\n","      <td>284807.0</td>\n","      <td>1.473120e-16</td>\n","      <td>0.734524</td>\n","      <td>-34.830382</td>\n","      <td>-0.228395</td>\n","      <td>-0.029450</td>\n","      <td>0.186377</td>\n","      <td>27.202839</td>\n","    </tr>\n","    <tr>\n","      <th>V22</th>\n","      <td>284807.0</td>\n","      <td>8.042109e-16</td>\n","      <td>0.725702</td>\n","      <td>-10.933144</td>\n","      <td>-0.542350</td>\n","      <td>0.006782</td>\n","      <td>0.528554</td>\n","      <td>10.503090</td>\n","    </tr>\n","    <tr>\n","      <th>V23</th>\n","      <td>284807.0</td>\n","      <td>5.282512e-16</td>\n","      <td>0.624460</td>\n","      <td>-44.807735</td>\n","      <td>-0.161846</td>\n","      <td>-0.011193</td>\n","      <td>0.147642</td>\n","      <td>22.528412</td>\n","    </tr>\n","    <tr>\n","      <th>V24</th>\n","      <td>284807.0</td>\n","      <td>4.456271e-15</td>\n","      <td>0.605647</td>\n","      <td>-2.836627</td>\n","      <td>-0.354586</td>\n","      <td>0.040976</td>\n","      <td>0.439527</td>\n","      <td>4.584549</td>\n","    </tr>\n","    <tr>\n","      <th>V25</th>\n","      <td>284807.0</td>\n","      <td>1.426896e-15</td>\n","      <td>0.521278</td>\n","      <td>-10.295397</td>\n","      <td>-0.317145</td>\n","      <td>0.016594</td>\n","      <td>0.350716</td>\n","      <td>7.519589</td>\n","    </tr>\n","    <tr>\n","      <th>V26</th>\n","      <td>284807.0</td>\n","      <td>1.701640e-15</td>\n","      <td>0.482227</td>\n","      <td>-2.604551</td>\n","      <td>-0.326984</td>\n","      <td>-0.052139</td>\n","      <td>0.240952</td>\n","      <td>3.517346</td>\n","    </tr>\n","    <tr>\n","      <th>V27</th>\n","      <td>284807.0</td>\n","      <td>-3.662252e-16</td>\n","      <td>0.403632</td>\n","      <td>-22.565679</td>\n","      <td>-0.070840</td>\n","      <td>0.001342</td>\n","      <td>0.091045</td>\n","      <td>31.612198</td>\n","    </tr>\n","    <tr>\n","      <th>V28</th>\n","      <td>284807.0</td>\n","      <td>-1.217809e-16</td>\n","      <td>0.330083</td>\n","      <td>-15.430084</td>\n","      <td>-0.052960</td>\n","      <td>0.011244</td>\n","      <td>0.078280</td>\n","      <td>33.847808</td>\n","    </tr>\n","    <tr>\n","      <th>Amount</th>\n","      <td>284807.0</td>\n","      <td>8.834962e+01</td>\n","      <td>250.120109</td>\n","      <td>0.000000</td>\n","      <td>5.600000</td>\n","      <td>22.000000</td>\n","      <td>77.165000</td>\n","      <td>25691.160000</td>\n","    </tr>\n","    <tr>\n","      <th>Class</th>\n","      <td>284807.0</td>\n","      <td>1.727486e-03</td>\n","      <td>0.041527</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           count          mean           std         min           25%  \\\n","Time    284807.0  9.481386e+04  47488.145955    0.000000  54201.500000   \n","V1      284807.0  3.918649e-15      1.958696  -56.407510     -0.920373   \n","V2      284807.0  5.682686e-16      1.651309  -72.715728     -0.598550   \n","V3      284807.0 -8.761736e-15      1.516255  -48.325589     -0.890365   \n","V4      284807.0  2.811118e-15      1.415869   -5.683171     -0.848640   \n","V5      284807.0 -1.552103e-15      1.380247 -113.743307     -0.691597   \n","V6      284807.0  2.040130e-15      1.332271  -26.160506     -0.768296   \n","V7      284807.0 -1.698953e-15      1.237094  -43.557242     -0.554076   \n","V8      284807.0 -1.893285e-16      1.194353  -73.216718     -0.208630   \n","V9      284807.0 -3.147640e-15      1.098632  -13.434066     -0.643098   \n","V10     284807.0  1.772925e-15      1.088850  -24.588262     -0.535426   \n","V11     284807.0  9.289524e-16      1.020713   -4.797473     -0.762494   \n","V12     284807.0 -1.803266e-15      0.999201  -18.683715     -0.405571   \n","V13     284807.0  1.674888e-15      0.995274   -5.791881     -0.648539   \n","V14     284807.0  1.475621e-15      0.958596  -19.214325     -0.425574   \n","V15     284807.0  3.501098e-15      0.915316   -4.498945     -0.582884   \n","V16     284807.0  1.392460e-15      0.876253  -14.129855     -0.468037   \n","V17     284807.0 -7.466538e-16      0.849337  -25.162799     -0.483748   \n","V18     284807.0  4.258754e-16      0.838176   -9.498746     -0.498850   \n","V19     284807.0  9.019919e-16      0.814041   -7.213527     -0.456299   \n","V20     284807.0  5.126845e-16      0.770925  -54.497720     -0.211721   \n","V21     284807.0  1.473120e-16      0.734524  -34.830382     -0.228395   \n","V22     284807.0  8.042109e-16      0.725702  -10.933144     -0.542350   \n","V23     284807.0  5.282512e-16      0.624460  -44.807735     -0.161846   \n","V24     284807.0  4.456271e-15      0.605647   -2.836627     -0.354586   \n","V25     284807.0  1.426896e-15      0.521278  -10.295397     -0.317145   \n","V26     284807.0  1.701640e-15      0.482227   -2.604551     -0.326984   \n","V27     284807.0 -3.662252e-16      0.403632  -22.565679     -0.070840   \n","V28     284807.0 -1.217809e-16      0.330083  -15.430084     -0.052960   \n","Amount  284807.0  8.834962e+01    250.120109    0.000000      5.600000   \n","Class   284807.0  1.727486e-03      0.041527    0.000000      0.000000   \n","\n","                 50%            75%            max  \n","Time    84692.000000  139320.500000  172792.000000  \n","V1          0.018109       1.315642       2.454930  \n","V2          0.065486       0.803724      22.057729  \n","V3          0.179846       1.027196       9.382558  \n","V4         -0.019847       0.743341      16.875344  \n","V5         -0.054336       0.611926      34.801666  \n","V6         -0.274187       0.398565      73.301626  \n","V7          0.040103       0.570436     120.589494  \n","V8          0.022358       0.327346      20.007208  \n","V9         -0.051429       0.597139      15.594995  \n","V10        -0.092917       0.453923      23.745136  \n","V11        -0.032757       0.739593      12.018913  \n","V12         0.140033       0.618238       7.848392  \n","V13        -0.013568       0.662505       7.126883  \n","V14         0.050601       0.493150      10.526766  \n","V15         0.048072       0.648821       8.877742  \n","V16         0.066413       0.523296      17.315112  \n","V17        -0.065676       0.399675       9.253526  \n","V18        -0.003636       0.500807       5.041069  \n","V19         0.003735       0.458949       5.591971  \n","V20        -0.062481       0.133041      39.420904  \n","V21        -0.029450       0.186377      27.202839  \n","V22         0.006782       0.528554      10.503090  \n","V23        -0.011193       0.147642      22.528412  \n","V24         0.040976       0.439527       4.584549  \n","V25         0.016594       0.350716       7.519589  \n","V26        -0.052139       0.240952       3.517346  \n","V27         0.001342       0.091045      31.612198  \n","V28         0.011244       0.078280      33.847808  \n","Amount     22.000000      77.165000   25691.160000  \n","Class       0.000000       0.000000       1.000000  "]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["card_df.describe().T"]},{"cell_type":"code","execution_count":53,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique classes in the dataset are : [0 1]\n"]}],"source":["# Unique class labels\n","print(f\"Unique classes in the dataset are : {np.unique(card_df['Class'])}\" )"]},{"cell_type":"code","execution_count":54,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["<AxesSubplot:xlabel='Class'>"]},"execution_count":54,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["card_df.groupby('Class')['Class'].count().plot.bar(logy=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Trasnformation\n","\n","Data transformation is one of the steps in data processing. We need to transform certain attributes value so that it makes sense in the further analysis. "]},{"cell_type":"code","execution_count":55,"metadata":{"trusted":true},"outputs":[],"source":["# Change the time attribute in day\n","card_df['Time'] = card_df['Time'].apply(lambda t: (t/3600) % 24 )"]},{"cell_type":"code","execution_count":56,"metadata":{"trusted":true},"outputs":[],"source":["# Sampling of data\n","normal_trans = card_df[card_df['Class'] == 0].sample(4000)\n","fraud_trans = card_df[card_df['Class'] == 1]"]},{"cell_type":"code","execution_count":57,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\bodar\\AppData\\Local\\Temp\\ipykernel_10220\\4007564440.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  reduced_set = normal_trans.append(fraud_trans).reset_index(drop=True)\n"]}],"source":["reduced_set = normal_trans.append(fraud_trans).reset_index(drop=True)"]},{"cell_type":"code","execution_count":58,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cleansed dataset shape : (4492, 31)\n"]}],"source":["print(f\"Cleansed dataset shape : {reduced_set.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Split the Dataset"]},{"cell_type":"code","execution_count":59,"metadata":{"trusted":true},"outputs":[],"source":["# Splitting the dataset into X and y features\n","y = reduced_set['Class']\n","X = reduced_set.drop('Class', axis=1)\n"]},{"cell_type":"code","execution_count":60,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of Features : (4492, 30) and Target: (4492,)\n"]}],"source":["print(f\"Shape of Features : {X.shape} and Target: {y.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Visualize the data with t-SNE\n","\n","TNSE(t-distributed Stochastic Neighbor Embedding) is one of the dimensionality reduction method other than PCA and SVD. This will supress some noise and speed up the computation of pairwise distance between samples. "]},{"cell_type":"code","execution_count":61,"metadata":{"trusted":true},"outputs":[],"source":["def dimensionality_plot(X, y):\n","    sns.set(style='whitegrid', palette='muted')\n","    # Initializing TSNE object with 2 principal components\n","    tsne = TSNE(n_components=2, random_state = 42)\n","    \n","    # Fitting the data\n","    X_trans = tsne.fit_transform(X)\n","    \n","    plt.figure(figsize=(12,8))\n","    \n","    plt.scatter(X_trans[np.where(y == 0), 0], X_trans[np.where(y==0), 1], marker='o', color='g', linewidth='1', alpha=0.8, label='Normal')\n","    plt.scatter(X_trans[np.where(y == 1), 0], X_trans[np.where(y==1), 1], marker='o', color='k', linewidth='1', alpha=0.8, label='Fraud')\n","    \n","    plt.legend(loc = 'best')\n","    \n","    plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Normalize and Scale the features"]},{"cell_type":"code","execution_count":62,"metadata":{"trusted":true},"outputs":[],"source":["scaler = RobustScaler().fit_transform(X)\n","\n","# Scaled data\n","X_scaled_normal = scaler[y == 0]\n","X_scaled_fraud = scaler[y == 1]"]},{"cell_type":"markdown","metadata":{},"source":["## Building Autoencoder Model"]},{"cell_type":"code","execution_count":63,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of the input data : 30\n"]}],"source":["print(f\"Shape of the input data : {X.shape[1]}\")"]},{"cell_type":"code","execution_count":64,"metadata":{"trusted":true},"outputs":[],"source":["# Input layer with a shape of features/columns of the dataset\n","input_layer = Input(shape = (X.shape[1], ))\n","\n","# Construct encoder network\n","encoded = Dense(100, activation= 'tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\n","encoded = Dense(50, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(encoded)\n","encoded = Dense(25, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(encoded)\n","encoded = Dense(12, activation = 'tanh', activity_regularizer=regularizers.l1(10e-5))(encoded)\n","encoded = Dense(6, activation='relu')(encoded)\n","\n","# Decoder network\n","decoded = Dense(12, activation='tanh')(encoded)\n","decoded = Dense(25, activation='tanh')(decoded)\n","decoded = Dense(50, activation='tanh')(decoded)\n","decoded = Dense(100, activation='tanh')(decoded)\n","\n","output_layer = Dense(X.shape[1], activation='relu')(decoded)\n","\n","# Building a model\n","auto_encoder = Model(input_layer, output_layer)"]},{"cell_type":"code","execution_count":65,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","100/100 [==============================] - 2s 4ms/step - loss: 2.1014 - val_loss: 1.3118\n","Epoch 2/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.1007 - val_loss: 1.3110\n","Epoch 3/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.1000 - val_loss: 1.3103\n","Epoch 4/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0993 - val_loss: 1.3096\n","Epoch 5/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0986 - val_loss: 1.3090\n","Epoch 6/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0980 - val_loss: 1.3083\n","Epoch 7/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0973 - val_loss: 1.3076\n","Epoch 8/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0967 - val_loss: 1.3070\n","Epoch 9/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0960 - val_loss: 1.3064\n","Epoch 10/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0954 - val_loss: 1.3058\n","Epoch 11/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0948 - val_loss: 1.3051\n","Epoch 12/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0942 - val_loss: 1.3046\n","Epoch 13/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0936 - val_loss: 1.3040\n","Epoch 14/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0931 - val_loss: 1.3034\n","Epoch 15/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0925 - val_loss: 1.3028\n","Epoch 16/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0920 - val_loss: 1.3023\n","Epoch 17/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0914 - val_loss: 1.3017\n","Epoch 18/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0909 - val_loss: 1.3012\n","Epoch 19/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0904 - val_loss: 1.3006\n","Epoch 20/20\n","100/100 [==============================] - 0s 2ms/step - loss: 2.0899 - val_loss: 1.3001\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1fe04d94250>"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["# Compile the auto encoder model\n","auto_encoder.compile(optimizer='adadelta', loss='mse')\n","\n","# Training the auto encoder model\n","auto_encoder.fit(X_scaled_normal, X_scaled_normal, batch_size=32, epochs=20, shuffle=True, validation_split=0.20)"]},{"cell_type":"markdown","metadata":{},"source":["## Using Autoencode to encode data"]},{"cell_type":"code","execution_count":66,"metadata":{"trusted":true},"outputs":[],"source":["latent_model = Sequential()\n","latent_model.add(auto_encoder.layers[0])\n","latent_model.add(auto_encoder.layers[1])\n","latent_model.add(auto_encoder.layers[2])\n","latent_model.add(auto_encoder.layers[3])\n","latent_model.add(auto_encoder.layers[4])"]},{"cell_type":"code","execution_count":67,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["125/125 [==============================] - 0s 1ms/step\n","16/16 [==============================] - 0s 1ms/step\n"]}],"source":["normal_tran_points = latent_model.predict(X_scaled_normal)\n","fraud_tran_points = latent_model.predict(X_scaled_fraud)\n","# Making as a one collection\n","encoded_X = np.append(normal_tran_points, fraud_tran_points, axis=0)\n","y_normal = np.zeros(normal_tran_points.shape[0])\n","y_fraud = np.ones(fraud_tran_points.shape[0])\n","encoded_y = np.append(y_normal, y_fraud, axis=0)\n"]},{"cell_type":"markdown","metadata":{},"source":["We can observe that the encoded fraud data points have been moved towards one cluster, whereas there are only few fraud transaction datapoints are there among the normal transaction data points. \n","\n","## Split into Train and Test"]},{"cell_type":"code","execution_count":68,"metadata":{"trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","X_enc_train, X_enc_test, y_enc_train, y_enc_test = train_test_split(encoded_X, encoded_y, test_size=0.3)"]},{"cell_type":"code","execution_count":69,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoded train data X: (3144, 12), Y: (3144,), X_test :(1348, 12), Y_test: (1348,)\n","Actual train & test data X: (3144, 30), Y: (3144, 30), X_test :(1348, 30), Y_test: (1348,)\n"]}],"source":["print(f\"Encoded train data X: {X_enc_train.shape}, Y: {y_enc_train.shape}, X_test :{X_enc_test.shape}, Y_test: {y_enc_test.shape}\")\n","print(f\"Actual train & test data X: {X_train.shape}, Y: {X_train.shape}, X_test :{X_test.shape}, Y_test: {y_test.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Non-linear Classifier"]},{"cell_type":"code","execution_count":70,"metadata":{"trusted":true},"outputs":[],"source":["# Instance of SVM\n","svc_clf = SVC()\n","\n","svc_clf.fit(X_train, y_train)\n","\n","svc_predictions = svc_clf.predict(X_test)"]},{"cell_type":"code","execution_count":71,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification report \n","               precision    recall  f1-score   support\n","\n","           0       0.92      1.00      0.96      1190\n","           1       1.00      0.37      0.54       158\n","\n","    accuracy                           0.93      1348\n","   macro avg       0.96      0.69      0.75      1348\n","weighted avg       0.93      0.93      0.91      1348\n","\n"]}],"source":["print(\"Classification report \\n {0}\".format(classification_report(y_test, svc_predictions)))"]},{"cell_type":"code","execution_count":72,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of SVC \n"," 0.93\n"]}],"source":["print(\"Accuracy of SVC \\n {:.2f}\".format(accuracy_score(y_test, svc_predictions)))"]},{"cell_type":"markdown","metadata":{},"source":["## Linear Classifier\n","\n","Now let's apply linear classifier to classify the data and observe the result. We will use **Logistic Regression** to build the model."]},{"cell_type":"code","execution_count":73,"metadata":{"trusted":true},"outputs":[],"source":["lr_clf = LogisticRegression()\n","\n","lr_clf.fit(X_enc_train, y_enc_train)\n","\n","# Predict the Test data\n","predictions = lr_clf.predict(X_enc_test)"]},{"cell_type":"code","execution_count":74,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification report \n","               precision    recall  f1-score   support\n","\n","         0.0       0.95      0.99      0.97      1197\n","         1.0       0.90      0.56      0.69       151\n","\n","    accuracy                           0.94      1348\n","   macro avg       0.92      0.77      0.83      1348\n","weighted avg       0.94      0.94      0.94      1348\n","\n"]}],"source":["print(\"Classification report \\n {0}\".format(classification_report(y_enc_test, predictions)))\n"]},{"cell_type":"code","execution_count":75,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy score is : 0.94\n"]}],"source":["print(\"Accuracy score is : {:.2f}\".format(accuracy_score(y_enc_test, predictions)))"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["## Conclusion\n","\n","In this analysis, we have found that Support Vector Machine classifier is able to classify the data upto **93%** without encoding and decoding. However, the effect of autoencoder comes when the data gets transformed from non-linear to linearly separable then linear classifier like **Logistic Regression** could perform in a better way.\n","\n","The accuracy score of Logistic Regression can go upto **97%**, this is something not happens too often in logistic algorithm. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"884e1d6e3878343ab46052a6891ef7ab71e3f75a35ce8ffb0394c4d1a4097302"}}},"nbformat":4,"nbformat_minor":4}
